{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Teoria pre-code**"
      ],
      "metadata": {
        "id": "40hwLTRoTLMm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clusterização, ou agrupamento, é uma técnica de análise de dados usada em mineração de dados e aprendizado de máquina. Seu objetivo é agrupar um conjunto de objetos de tal forma que objetos no mesmo grupo (ou cluster) sejam mais semelhantes entre si do que com aqueles em outros grupos, de acordo com algum critério de semelhança.\n",
        "\n",
        "Em outras palavras, a clusterização busca encontrar estruturas intrínsecas nos dados, identificando grupos naturais ou segmentos dentro de um conjunto de dados, mesmo sem a necessidade de rótulos prévios. Essa técnica é amplamente utilizada em diversas áreas, como reconhecimento de padrões, análise de dados, mineração de textos, biologia, entre outras.\n",
        "\n",
        "Existem vários algoritmos de clusterização, cada um com suas próprias características e aplicabilidades, incluindo o algoritmo k-means, DBSCAN, entre outros. A escolha do algoritmo depende das características dos dados e dos objetivos específicos do problema em questão."
      ],
      "metadata": {
        "id": "CLwe4TzhTSYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table border=\"1\" cellpadding=\"5\">\n",
        "  <thead>\n",
        "    <tr>\n",
        "      <th>Método de Clusterização</th>\n",
        "      <th>Como é Calculado</th>\n",
        "      <th>Avaliação de Correção</th>\n",
        "      <th>Quando Utilizar</th>\n",
        "      <th>Exemplo de Utilização</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <td>KMeans</td>\n",
        "      <td>Agrupa os pontos em k clusters, minimizando a soma dos quadrados das distâncias dos pontos ao centroide de seus clusters.</td>\n",
        "      <td>Variabilidade intra-cluster, Coesão e Separação</td>\n",
        "      <td>Quando se tem uma noção prévia do número de clusters e os clusters são convexos.</td>\n",
        "      <td>Segmentação de clientes com base em seus padrões de compra.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Affinity Propagation</td>\n",
        "      <td>Identifica os \"exemplares\" entre os pontos e então propaga afinidades entre todos os pares de pontos.</td>\n",
        "      <td>Silhueta, Cohesão, Interpretabilidade</td>\n",
        "      <td>Quando não se sabe o número de clusters a priori e quando há um grande número de pontos de dados e a estrutura de similaridade é complexa.</td>\n",
        "      <td>Segmentação de redes sociais com base em interações entre usuários.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>BIRCH</td>\n",
        "      <td>Utiliza uma árvore hierárquica de clusters (Cluster Feature Tree) para dividir os dados em subclusters hierárquicos.</td>\n",
        "      <td>Coesão, Separação</td>\n",
        "      <td>Quando se tem um grande volume de dados, pois é escalável e eficiente, e quando a estrutura dos clusters é hierárquica.</td>\n",
        "      <td>Análise de logs de servidores para identificar padrões de tráfego.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>DBSCAN</td>\n",
        "      <td>Baseado na densidade dos pontos, agrupa regiões densas de pontos e identifica pontos isolados como ruído.</td>\n",
        "      <td>Silhueta, Separabilidade, Tamanho dos Clusters</td>\n",
        "      <td>Quando os clusters têm formas arbitrárias e tamanhos diferentes, e quando há ruído nos dados.</td>\n",
        "      <td>Identificação de regiões de alta densidade em um conjunto de dados geoespaciais.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Mean Shift</td>\n",
        "      <td>Move iterativamente os pontos para a região de maior densidade de pontos até convergir para os centros dos clusters.</td>\n",
        "      <td>Silhueta, Separabilidade, Coesão</td>\n",
        "      <td>Quando não se sabe o número de clusters a priori, os clusters não são necessariamente esféricos e a densidade dos pontos varia.</td>\n",
        "      <td>Segmentação de uma imagem para identificar objetos.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>OPTICS</td>\n",
        "      <td>Identifica os clusters e suas estruturas através da construção de um \"reachability plot\", que mostra a densidade dos pontos e a conectividade entre eles.</td>\n",
        "      <td>Silhueta, Separabilidade, Estrutura do Cluster</td>\n",
        "      <td>Quando não se sabe o número de clusters a priori, quando os clusters têm formas arbitrárias e tamanhos diferentes, e quando há ruído nos dados.</td>\n",
        "      <td>Análise de tráfego de rede para identificar anomalias ou padrões de uso.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Spectral Clustering</td>\n",
        "      <td>Usa a estrutura espectral dos dados para realizar a clusterização, convertendo os dados em um espaço de menor dimensão e aplicando algoritmos de clustering nesse espaço.</td>\n",
        "      <td>Coesão, Separação, Estrutura do Cluster</td>\n",
        "      <td>Quando os clusters não são necessariamente convexos, a topologia dos dados é importante e quando os dados são representados como um grafo.</td>\n",
        "      <td>Segmentação de imagens médicas para identificação de tecidos e órgãos.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Gaussian Mixture Model (GMM)</td>\n",
        "      <td>Assume que os dados são gerados a partir de uma mistura de várias distribuições gaussianas e usa o algoritmo de Expectation-Maximization (EM) para estimar os parâmetros das distribuições.</td>\n",
        "      <td>Coesão, Separação, Estrutura do Cluster</td>\n",
        "      <td>Quando os clusters têm distribuições gaussianas e podem ter sobreposição, ou quando os dados não podem ser facilmente separados linearmente.</td>\n",
        "      <td>Segmentação de clientes com base em características demográficas e comportamentais.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Agglomerative Clustering</td>\n",
        "      <td>Constrói uma hierarquia de clusters unindo pares de clusters próximos, iterativamente.</td>\n",
        "      <td>Coesão, Separação, Estrutura do Cluster</td>\n",
        "      <td>Quando a estrutura hierárquica dos clusters é importante e quando se deseja entender a relação entre os clusters em diferentes níveis de granularidade.</td>\n",
        "      <td>Análise de expressão gênica para identificação de padrões de regulação e relacionamentos entre genes.</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>"
      ],
      "metadata": {
        "id": "tAN2az4sscx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Libraries principais**\n",
        "\n"
      ],
      "metadata": {
        "id": "OlEeo27Av8Dr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "52DY4RrOhTWi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset**\n"
      ],
      "metadata": {
        "id": "KnE0Y7WGwAV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonte dos dados:  https://www.kaggle.com/datasets/shwetabh123/mall-customers\n",
        "df = pd.read_csv('/content/Mall_Customers.csv')"
      ],
      "metadata": {
        "id": "Cz8RGwjuCNiE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "hSbVArb8CcZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fazendo feature encoding\n",
        "df1 = pd.get_dummies(df, columns=['Genre'], prefix=['Genre'])"
      ],
      "metadata": {
        "id": "qI4OOujjD_bj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1"
      ],
      "metadata": {
        "id": "n2tFhiMYEF8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#verificando se não há celulas vazias\n",
        "\n",
        "df1.isna().sum()"
      ],
      "metadata": {
        "id": "QQx2S20XCtTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df1.drop(['CustomerID'], axis= 1)"
      ],
      "metadata": {
        "id": "XfgSYB5GGwVR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##split data"
      ],
      "metadata": {
        "id": "78TSCC8HwK52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividindo o DataFrame em treinamento (70%) e teste (30%)\n",
        "train_data, test_data = train_test_split(df1, test_size=0.3, random_state=42)\n",
        "\n",
        "# Dividindo o conjunto de treinamento em teste (50%) e validação (50%) = 15/15\n",
        "test_data, validation_data = train_test_split(test_data, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "Oo_lJ78BDhBK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checkando divisão\n",
        "Ctotal = df1['Age'].count()\n",
        "Ctrain = train_data['Age'].count()\n",
        "Ctest = test_data['Age'].count()\n",
        "Cvalid = validation_data['Age'].count()\n",
        "\n",
        "print (Ctrain/Ctotal)\n",
        "print (Ctest/Ctotal)\n",
        "print (Cvalid/Ctotal)"
      ],
      "metadata": {
        "id": "QomeaerKDlzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Remoção de outliers por IQR"
      ],
      "metadata": {
        "id": "hbAmlqeRxpX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcule o primeiro quartil (Q1)\n",
        "q1 = train_data.quantile(0.25)\n",
        "\n",
        "# Calcule o terceiro quartil (Q3)\n",
        "q3 = train_data.quantile(0.75)\n",
        "\n",
        "# Calcule o IQR (Intervalo Interquartil)\n",
        "iqr = q3 - q1\n",
        "\n",
        "print(f\"Q1: {q1}\")\n",
        "print(f\"Q3: {q3}\")\n",
        "print(f\"IQR: {iqr}\")"
      ],
      "metadata": {
        "id": "1IpJ2UInDxNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = 1.5   #constante\n",
        "LI = q1 - c*iqr   #limite inferior\n",
        "LS = q3 + c*iqr   #limite superior\n",
        "\n",
        "print(f\"LI: {LI}\")\n",
        "print(f\"LS: {LS}\")"
      ],
      "metadata": {
        "id": "M2kZNw8HF0XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fazer remoção de outlier por coluna\n",
        "trainlimpo = train_data[(train_data['Age'] >= LI['Age']) & (train_data['Age'] <= LS['Age'])]\n",
        "print(trainlimpo.shape)\n",
        "trainlimpo = trainlimpo[(trainlimpo['Annual Income (k$)'] >= LI['Annual Income (k$)']) & (trainlimpo['Annual Income (k$)'] <= LS['Annual Income (k$)'])]\n",
        "print(trainlimpo.shape)\n",
        "trainlimpo = trainlimpo[(trainlimpo['Spending Score (1-100)'] >= LI['Spending Score (1-100)']) & (trainlimpo['Spending Score (1-100)'] <= LS['Spending Score (1-100)'])]\n",
        "print(trainlimpo.shape)"
      ],
      "metadata": {
        "id": "GO7VsjEfHobt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalizando os dados\n"
      ],
      "metadata": {
        "id": "rO_Cr1hmx7un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "medtrain = trainlimpo.mean()\n",
        "stdtrain = trainlimpo.std()\n",
        "Ctreino = (trainlimpo - medtrain)/ stdtrain"
      ],
      "metadata": {
        "id": "xP3Yy7KwISze"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ctreino"
      ],
      "metadata": {
        "id": "JdTKl7bQDSo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#verificando se não há celulas vazias\n",
        "\n",
        "trainlimpo.isna().sum()"
      ],
      "metadata": {
        "id": "oXGyENpbIM82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificando desvio padrão normalizado =>>> Tem q ser igual ou proximo 1.\n",
        "Ctreino.std()"
      ],
      "metadata": {
        "id": "sNYNdQ2GGnGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Media normalizada =>>>>>> Tem que ser igual a 0\n",
        "\n",
        "round(Ctreino.mean(),7)"
      ],
      "metadata": {
        "id": "4G5OXhBDIvXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ctreino"
      ],
      "metadata": {
        "id": "6olS7m707Hsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Cvad = (validation_data - medtrain)/ stdtrain\n",
        "Ctest = (test_data - medtrain)/ stdtrain"
      ],
      "metadata": {
        "id": "dGNq7sFzJDYt"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Cvad"
      ],
      "metadata": {
        "id": "loUoCvHQJG20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Grupo Para testar novos inputs"
      ],
      "metadata": {
        "id": "2SIG2PBscUKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pessoasnovas = {\n",
        "    'Genre': ['Male', 'Female', 'Male', 'Female', 'Male'],\n",
        "    'Age': [25, 30, 35, 28, 40],\n",
        "    'Annual Income (k$)': [50, 60, 45, 70, 80],\n",
        "    'Spending Score (1-100)': [70, 50, 75, 60, 45],\n",
        "    }\n",
        "\n",
        "pessoasnovas = pd.DataFrame(pessoasnovas)\n",
        "pessoasnovas = pd.get_dummies(pessoasnovas, columns=['Genre'], prefix=['Genre'])"
      ],
      "metadata": {
        "id": "7kaTjfs7cPao"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Clusterização**"
      ],
      "metadata": {
        "id": "BKy2UzQZyMiZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KMeans\n"
      ],
      "metadata": {
        "id": "YSNS3lJlyI0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explicação"
      ],
      "metadata": {
        "id": "dIwTo7idxbQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O algoritmo K-means é uma técnica popular de clusterização que divide um conjunto de dados em K grupos, onde K é um número definido previamente pelo usuário. Aqui está uma explicação simplificada de como o algoritmo K-means funciona:\n",
        "\n",
        "No algoritmo K-means, você define a **quantidade de clusters K** antes de executar o algoritmo. A escolha do número ideal de clusters depende do seu conhecimento do problema e dos dados.\n",
        "\n",
        "Existem várias técnicas para determinar o número ideal de clusters, como o método do cotovelo (elbow method), coeficiente de silhueta (silhouette coefficient), entre outros. Estes métodos ajudam a identificar o número ótimo de clusters com base nas características dos seus dados.\n",
        "\n",
        "Depois de determinar o número de clusters, o algoritmo K-means atribui cada ponto de dado ao cluster mais próximo do centroide correspondente.\n",
        "\n",
        "O <font color=\"red\">**Centroid** </font> é o ponto central de um cluster. O algoritmo atualiza iterativamente a posição dos centroides até que os clusters estejam otimizados.\n",
        "\n",
        "\n",
        "**1 - Inicialização**: O algoritmo começa selecionando aleatoriamente K **(n_clusters)** pontos <font color=\"red\">**(Centroid)** </font>\n",
        " no espaço de dados para representar os centros iniciais dos clusters.\n",
        "\n",
        "**2- Atribuição de pontos aos clusters**: Cada ponto de dado <font color=\"blue\">**(sample)** </font> é então atribuído ao cluster cujo centro é mais próximo, com base na distância euclidiana, por exemplo.\n",
        "\n",
        "**3- Recálculo dos centros dos clusters**: Depois que todos os pontos são atribuídos a um cluster, os centros dos clusters são recalculados, ajustando suas posições para serem a média dos pontos que estão dentro de cada cluster.\n",
        "\n",
        "**4- Repetição**: Os passos 2 e 3 são repetidos até que os centros dos clusters não mudem significativamente entre as iterações ou até que um critério de parada seja alcançado **(max_iter)**.\n",
        "\n",
        "Geralmente, um critério de parada pode ser um número máximo de iterações ou quando a mudança nos centros dos clusters é pequena o suficiente.\n",
        "\n",
        "\n",
        "<img src=\"https://www.unioviedo.es/compnum/labs/PYTHON/d1.png\" alt=\"Imagem\" width=\"450\" height=\"300\">\n",
        "\n",
        "**5- Convergência**: Quando o algoritmo convergiu, os centros dos clusters permanecem estáveis e os pontos de dados estão bem atribuídos a cada cluster **(tol)**.\n",
        "\n",
        "</p>\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/v2/resize:fit:709/1*JsfEdbXKwJw_Euprvx17KA.png\" alt=\"Imagem\" width=\"450\" height=\"300\">\n",
        "\n",
        "O resultado final do algoritmo K-means é uma partição dos dados em K clusters, onde cada ponto de dado pertence a um e apenas um cluster. Este algoritmo é eficiente e fácil de implementar, porém, pode convergir para um ótimo local dependendo da inicialização dos centros dos clusters e pode ser sensível à presença de outliers nos dados.\n"
      ],
      "metadata": {
        "id": "TTKMLVNaxkmR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parametros"
      ],
      "metadata": {
        "id": "ybAJhnu11Jey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "class sklearn.cluster.KMeans(n_clusters=8, *, init='k-means++', n_init='auto', max_iter=300, tol=0.0001, verbose=0, random_state=None, copy_x=True, algorithm='lloyd')\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "**Número de clusters (n_clusters)**: Este é o parâmetro mais fundamental do algoritmo K-means. Define o número de clusters nos quais o conjunto de dados será dividido.\n",
        "\n",
        "**Inicialização (init):** Este parâmetro determina o método de inicialização dos centróides dos clusters. Pode ser definido como \"k-means++\", que usa um método inteligente para escolher os centróides iniciais de forma a melhorar a convergência, ou \"random\", que escolhe aleatoriamente os centróides iniciais.\n",
        "\n",
        "**Número máximo de iterações (max_iter)**: Define o número máximo de iterações permitidas antes de o algoritmo ser considerado convergido. Se os centróides não convergirem antes de atingir esse número de iterações, o algoritmo para. Isso ajuda a evitar loops infinitos. O valor padrão geralmente é 300.\n",
        "\n",
        "**Critério de convergência (tol)**: Este é o critério de convergência que controla o quão próximo os centróides devem estar uns dos outros para considerar que o algoritmo convergiu. Se a mudança nos centróides entre duas iterações consecutivas for menor que esse valor, o algoritmo para. O valor padrão é geralmente 1e-4.\n",
        "\n",
        "**Número de inicializações (n_init)**: Define o número de vezes que o algoritmo será executado com diferentes centróides iniciais. Isso ajuda a melhorar a robustez do algoritmo, pois o resultado final pode depender da escolha inicial dos centróides. O resultado final é o melhor de todas as execuções. O valor padrão geralmente é 10.\n"
      ],
      "metadata": {
        "id": "r8-buml91M-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code"
      ],
      "metadata": {
        "id": "mXHUR2Ktxe4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Melhores metricas"
      ],
      "metadata": {
        "id": "1_rvXFlQQUJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "def treinakmeans(k,Ctreino):\n",
        "    best = None\n",
        "    bestin = np.Inf\n",
        "    for _ in range(0,10):\n",
        "        kmeans = KMeans(n_clusters=k, n_init=10)  #n_init é o numero de vezes que vai correr com numero do centroid diferentes\n",
        "        kmeans.fit(Ctreino)\n",
        "        inertia = kmeans.inertia_\n",
        "        if inertia < bestin:\n",
        "            best = kmeans\n",
        "            bestin = inertia\n",
        "    return best, bestin"
      ],
      "metadata": {
        "id": "mvF5fCcUJhLF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crie uma lista para armazenar os valores de inércia\n",
        "inertia = []\n",
        "siluetas = []\n",
        "n = 11\n",
        "\n",
        "# Execute o K-Means para diferentes valores de k\n",
        "for k in tqdm(range(2, n)):  # Teste valores de k de 1 a 10 (você pode ajustar o intervalo)\n",
        "    kmeans,IN = treinakmeans(k,Ctreino)\n",
        "    inertia.append(IN)\n",
        "    labels= kmeans.predict(Cvad)\n",
        "    silhouette_avg = silhouette_score(Cvad, labels)\n",
        "    siluetas.append(silhouette_avg)"
      ],
      "metadata": {
        "id": "Uvp9yUukJVyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crie um gráfico do Elbow Point\n",
        "plt.plot(inertia, marker='o')\n",
        "plt.xticks(range(len(inertia)),range(len(inertia)) )\n",
        "plt.xlabel('Número de Clusters (k) -2')\n",
        "plt.ylabel('Inércia')\n",
        "plt.title('Método Elbow para Determinação de k')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xYFRb0Om9gVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # Trace o gráfico da silhueta\n",
        "plt.plot(siluetas, marker='o')\n",
        "plt.xlabel('Número de Clusters (k)-2')\n",
        "plt.xticks(range(len(siluetas)),range(len(siluetas)) )\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Avaliação do Número de Clusters ')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RbiMDmhgJWg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Clusterizando"
      ],
      "metadata": {
        "id": "-QRqcPQEQdHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(n_clusters=6, n_init=10)\n",
        "kmeans_model = kmeans.fit(Ctreino)\n",
        "centroids = kmeans.cluster_centers_\n",
        "labels = kmeans.labels_"
      ],
      "metadata": {
        "id": "Xv2diaxFJzAJ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adicione os rótulos dos clusters ao DataFrame\n",
        "Ctreino2 = trainlimpo.copy()\n",
        "Ctreino2['Cluster'] = kmeans.labels_\n",
        "Ctreino2['Gasto'] = Ctreino2['Spending Score (1-100)']/Ctreino2['Annual Income (k$)']\n",
        "\n",
        "# Criar o gráfico de dispersão\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plotar os pontos para o cluster atual com cor diferente\n",
        "plt.scatter(Ctreino2['Age'], Ctreino2['Gasto'],c=Ctreino2['Cluster'] )\n",
        "\n",
        "plt.ylabel('Gasto medio')\n",
        "plt.xlabel('Cluster')\n",
        "plt.title('Distribution of Clusters by Spending Score')\n",
        "plt.show()\n",
        "\n",
        "#Age\n",
        "#Annual Income (k$)\n",
        "#Spending Score (1-100)\n",
        "#Gender_Female\n",
        "#Gender_Male"
      ],
      "metadata": {
        "id": "wXGmMYvUKyTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plote os resultados em 3D\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Atribua as colunas aos eixos x, y e z\n",
        "ax.scatter(Ctreino2['Age'], Ctreino2['Gasto'], Ctreino2['Genre_Male'], c=Ctreino2['Cluster'], cmap='viridis', s=50, alpha=0.6)\n",
        "\n",
        "# Rotule os eixos\n",
        "ax.set_xlabel('idade')\n",
        "ax.set_ylabel('gasto')\n",
        "ax.set_zlabel('receita')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lxJ4kdPUM0lU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Aplicando a clusterização aos grupos de validação e teste\n",
        "labels_validacao = kmeans.predict(Cvad)\n",
        "labels_teste = kmeans.predict(Ctest)"
      ],
      "metadata": {
        "id": "JQJhTT5gM4FE"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Cvad['Cluster'] = labels_validacao\n",
        "validation_data['Cluster'] = labels_validacao\n",
        "Ctest['Cluster'] = labels_teste"
      ],
      "metadata": {
        "id": "jAOsueT6K6G7"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ctest"
      ],
      "metadata": {
        "id": "DuSoeAk_K70M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pontuação de silhueta quanto mais proximo de 1 melhor\n",
        "silhouette_score_validacao = silhouette_score(Cvad, labels_validacao)\n",
        "print(\"Pontuação de silhueta do conjunto de validação:\", silhouette_score_validacao)\n",
        "\n",
        "# Pontuação Calinski-Harabasz quanto maior melhor.\n",
        "calinski_harabasz_score_validacao = calinski_harabasz_score(Cvad, labels_validacao)\n",
        "print(\"Pontuação Calinski-Harabasz do conjunto de validação:\", calinski_harabasz_score_validacao)"
      ],
      "metadata": {
        "id": "Esw6KVl5Ph-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#grupos 0 e 5 gasta muito\n",
        "#grupo 2 e 1 gasta pouco\n",
        "#grupo 4 e 3 gasta medio\n",
        "# impar mulher par homem"
      ],
      "metadata": {
        "id": "o1VfzOgoXiFD"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testando clusterização com novas inserções"
      ],
      "metadata": {
        "id": "aQ3t81pEFB3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clusterizar(data,medtrain,stdtrain,kmeans):\n",
        "  df1 = data\n",
        "  df1 = (df1 - medtrain)/ stdtrain\n",
        "  labels = kmeans.predict(df1)\n",
        "  print(labels)\n",
        "  return labels"
      ],
      "metadata": {
        "id": "LbmZ0gmMYigv"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pessoasnovas"
      ],
      "metadata": {
        "id": "OWLLig7e5k0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = []\n",
        "for index, row in pessoasnovas.iterrows():\n",
        "  labels.append(clusterizar(pessoasnovas.iloc[[index]],medtrain,stdtrain,kmeans)[0])\n",
        "\n",
        "pessoasnovas2 = pessoasnovas.copy()\n",
        "pessoasnovas2['Cluster'] = labels"
      ],
      "metadata": {
        "id": "lSLynLbUaABD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pessoasnovas2"
      ],
      "metadata": {
        "id": "47VmFFtaaSBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AffinityPropagation\n"
      ],
      "metadata": {
        "id": "IePQU_45OgYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explicação"
      ],
      "metadata": {
        "id": "1LN396gmzf32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O algoritmo Affinity Propagation é uma técnica de clusterização que **não requer que o número de clusters** seja especificado a priori. Ele funciona encontrando automaticamente os exemplos mais representativos no conjunto de dados, e atribuindo outros pontos a esses exemplares com base em sua similaridade.\n",
        "\n",
        "**Similaridade entre pontos**: O algoritmo começa calculando uma matriz de similaridade entre todos os pares de pontos de dados. A similaridade pode ser medida de várias maneiras, como a distância euclidiana ou uma função de similaridade específica ao domínio do problema.\n",
        "\n",
        "**Escolha dos exemplares iniciais:** Cada ponto de dado é considerado como um possível exemplar inicial. **(preference)**\n",
        "\n",
        "**Mensagens de responsabilidade e disponibilidade**: O algoritmo itera entre duas mensagens entre todos os pares de pontos **(damping)**:\n",
        "\n",
        "*   **Mensagem de responsabilidade**: Reflete a evidência de quanto um ponto deve ser um exemplar com base na comparação com outros pontos.\n",
        "*   **Mensagem de disponibilidade**: Reflete a evidência de quanto um ponto deve escolher outro ponto como seu exemplar com base na comparação com outros pontos.\n",
        "\n",
        "**Atualização das mensagens**: As mensagens de responsabilidade e disponibilidade são atualizadas em cada iteração com base nas mensagens anteriores e na similaridade entre os pontos **(Convergência)**.\n",
        "\n",
        "**Determinação dos clusters**: O algoritmo continua iterando até convergir para um conjunto de exemplares e pontos atribuídos a esses exemplares. Os clusters são então formados com base nessas atribuições.\n",
        "\n",
        "\n",
        "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSUK0tObaVWkQ_YZKdyJY1c1ulEZUTywUflsb1bRgmS__Z89HcLTZicUfL1q_kJ4snhxDs&usqp=CAUg\" alt=\"Imagem\" width=\"450\" height=\"300\">\n",
        "\n",
        "O algoritmo Affinity Propagation é poderoso, mas pode ser computacionalmente caro para grandes conjuntos de dados devido à sua complexidade quadrática em relação ao número de pontos. No entanto, é útil quando o número de clusters não é conhecido a priori e quando os dados têm uma estrutura complexa."
      ],
      "metadata": {
        "id": "ZL7X8msBzlgn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parametros"
      ],
      "metadata": {
        "id": "HeCCNBIQyLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "class sklearn.cluster.AffinityPropagation(*, damping=0.5, max_iter=200, convergence_iter=15, copy=True, preference=None, affinity='euclidean', verbose=False, random_state=None)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "O algoritmo de Affinity Propagation possui alguns parâmetros que podem ser ajustados para controlar seu comportamento. Os principais parâmetros incluem:\n",
        "\n",
        "**Preferência (preference)**: Este parâmetro controla a quantidade de exemplos que são escolhidos como exemplares iniciais. Quanto maior o valor da preferência, mais exemplos serão escolhidos como exemplares iniciais. Se não for especificado, a preferência é definida como a mediana da matriz de similaridade.\n",
        "\n",
        "**Convergência (convergence_iter)**: Define o número máximo de iterações permitidas antes de o algoritmo ser considerado convergido. Se não for especificado, o padrão é 200.\n",
        "\n",
        "**Amortecimento (damping)**: Este parâmetro controla o fator de amortecimento aplicado durante a atualização das mensagens de responsabilidade e disponibilidade. Um fator de amortecimento menor pode acelerar a convergência, mas também pode tornar o algoritmo instável. O valor padrão é 0.5.\n",
        "\n",
        "Utilize métricas de avaliação de clusterização, como coeficiente de silhueta, índice Davies-Bouldin, índice de Calinski-Harabasz, entre outros, para comparar e avaliar os resultados obtidos com diferentes conjuntos de parâmetros"
      ],
      "metadata": {
        "id": "32Y6ebDayPo9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code"
      ],
      "metadata": {
        "id": "bahuDxnrzfrD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Melhores metricas"
      ],
      "metadata": {
        "id": "vWZhuF05QP8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AffinityPropagation\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
        "\n",
        "def treina_affinity_propagation(X):\n",
        "    best_model = None\n",
        "    best_silhouette = -1  # Inicializamos com o valor mínimo possível de silhouette score\n",
        "    best_cohesion = float('inf')  # Inicializamos com o valor máximo possível de coesão\n",
        "    best_interpretability = -1  # Inicializamos com o valor mínimo possível de interpretabilidade\n",
        "    best_n_clusters = -1  # Inicializamos com o valor mínimo possível de número de clusters\n",
        "\n",
        "    # Tente diferentes configurações para encontrar o melhor modelo\n",
        "    for _ in range(10):  # Faz 10 tentativas\n",
        "        model = AffinityPropagation(damping=0.9)  # Pode ajustar os parâmetros conforme necessário\n",
        "        labels = model.fit_predict(X)\n",
        "        n_clusters = len(model.cluster_centers_indices_)\n",
        "        silhouette = silhouette_score(X, labels)\n",
        "        cohesion = calinski_harabasz_score(X, labels)\n",
        "        interpretability = len(model.cluster_centers_indices_)\n",
        "\n",
        "        # Se as métricas forem melhores que as atuais, atualizamos o melhor modelo e as métricas correspondentes\n",
        "        if silhouette > best_silhouette:\n",
        "            best_model = model\n",
        "            best_silhouette = silhouette\n",
        "            best_cohesion = cohesion\n",
        "            best_interpretability = interpretability\n",
        "            best_n_clusters = n_clusters\n",
        "        if cohesion < best_cohesion:\n",
        "            best_cohesion = cohesion\n",
        "            best_silhouette = silhouette\n",
        "            best_interpretability = interpretability\n",
        "            best_n_clusters = n_clusters\n",
        "        if interpretability > best_interpretability:\n",
        "            best_interpretability = interpretability\n",
        "            best_silhouette = silhouette\n",
        "            best_cohesion = cohesion\n",
        "            best_n_clusters = n_clusters\n",
        "\n",
        "    return best_model, best_n_clusters, best_silhouette, best_cohesion, best_interpretability\n"
      ],
      "metadata": {
        "id": "Lp3K-JQzGpEZ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model, best_n_clusters, best_silhouette, best_cohesion, best_interpretability = treina_affinity_propagation(Ctreino)\n",
        "\n",
        "print(f'melhor modelo: {best_model}')\n",
        "print(f'Número de clusters: {best_n_clusters}')\n",
        "print(f'Silhueta: {best_silhouette}')\n",
        "print(f'Coesão: {best_cohesion}')\n",
        "print(f'Interpretabilidade: {best_interpretability}')"
      ],
      "metadata": {
        "id": "I1r81rkKGu-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def treina_affinity_propagation(Ctreino, preference=None):\n",
        "    af = AffinityPropagation(preference=preference)\n",
        "    af.fit(Ctreino)\n",
        "    return af\n",
        "\n",
        "# Crie uma lista para armazenar os valores de inércia e silhueta\n",
        "silhouettes = []\n",
        "num = 100\n",
        "\n",
        "# Execute o Affinity Propagation para diferentes valores de preferência\n",
        "for preference in tqdm(np.linspace(-100, -1, num)):\n",
        "    af = treina_affinity_propagation(Ctreino, preference=preference)\n",
        "    labels = af.predict(Cvad)\n",
        "    silhouette_avg = silhouette_score(Cvad, labels)\n",
        "    silhouettes.append(silhouette_avg)"
      ],
      "metadata": {
        "id": "S8Qk5wU489v6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trace o gráfico da silhueta\n",
        "plt.plot(silhouettes, marker='o')\n",
        "plt.xlabel('Número de Clusters (k)-2')\n",
        "plt.xticks(range(len(silhouettes)),range(len(silhouettes)) )\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Avaliação do Número de Clusters ')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "94F0E0b392vV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Clusterizando"
      ],
      "metadata": {
        "id": "MaR0keSlQFMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AffinityPropagation\n",
        "# Crie o objeto de clusterização\n",
        "af = AffinityPropagation()\n",
        "\n",
        "# Realize a clusterização\n",
        "clusters = af.fit_predict(Ctreino)\n",
        "\n",
        "# Adicione os clusters de volta ao DataFrame original\n",
        "Ctreino2 = Ctreino.copy()\n",
        "Ctreino2['Cluster'] = clusters\n",
        "Ctreino2"
      ],
      "metadata": {
        "id": "bZZm0F3cOi3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Novos Dados"
      ],
      "metadata": {
        "id": "2sCPK8iOQGs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clusterizar(data,medtrain,stdtrain,af):\n",
        "  df1 = data\n",
        "  df1 = (df1 - medtrain)/ stdtrain\n",
        "  labels = af.predict(df1)\n",
        "  return labels"
      ],
      "metadata": {
        "id": "VShyECXbIdum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = []\n",
        "for index, row in pessoasnovas.iterrows():\n",
        "  labels.append(clusterizar(pessoasnovas.iloc[[index]],medtrain,stdtrain,af)[0])\n",
        "\n",
        "pessoasnovas2 = pessoasnovas.copy()\n",
        "pessoasnovas2['Cluster'] = labels\n",
        "\n",
        "pessoasnovas2"
      ],
      "metadata": {
        "id": "odehNa9uIogT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BIRCH"
      ],
      "metadata": {
        "id": "07hdYCcuQKrG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explicação"
      ],
      "metadata": {
        "id": "44xt7Gxnzo1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O algoritmo **BIRCH** (Balanced Iterative Reducing and Clustering using Hierarchies) é uma técnica de clusterização que se destaca por sua eficiência computacional e capacidade de lidar com grandes conjuntos de dados.\n",
        "\n",
        "**Construção da estrutura de árvore (Tree Structure)**: O algoritmo começa construindo uma estrutura de árvore hierárquica. Esta estrutura de árvore é chamada de CF Tree (Clustering Feature Tree) e é usada para representar o conjunto de dados de forma compacta e eficiente.\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Gzzzv2MmddlUla_958pzqg.jpeg\" alt=\"Imagem\" width=\"500\" height=\"400\">\n",
        "\n",
        "**Agrupamento incremental dos dados**: À medida que novos pontos de dados são apresentados ao algoritmo, eles são inseridos na estrutura da árvore. O algoritmo atualiza dinamicamente as estatísticas (como o número de pontos, a média e a variância) de cada nó da árvore para refletir os novos dados.\n",
        "\n",
        "**Fusão de subclusters:** Periodicamente, o algoritmo verifica os nós da árvore e funde subclusters semelhantes para reduzir a complexidade da árvore e manter a eficiência do algoritmo.\n",
        "\n",
        "**Identificação de clusters**: Após a construção da árvore, o algoritmo passa por ela para identificar os clusters finais. Ele faz isso percorrendo os nós da árvore e combinando os subclusters semelhantes em clusters maiores, com base em um limiar de similaridade definido pelo usuário.\n",
        "\n",
        "**Atribuição de pontos aos clusters**: Finalmente, os pontos de dados são atribuídos aos clusters identificados com base em sua proximidade com os centros dos clusters.\n",
        "\n",
        "O algoritmo **BIRCH** é particularmente eficaz para grandes conjuntos de dados, pois permite uma representação compacta dos dados e opera de maneira incremental, sem a necessidade de armazenar todos os pontos de dados na memória ao mesmo tempo. Isso o torna adequado para aplicações onde os dados são muito grandes para caber na memória principal do computador.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bv70YGfgzoqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parametros"
      ],
      "metadata": {
        "id": "jAwkFZ9EmzLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "class sklearn.cluster.Birch(*, threshold=0.5, branching_factor=50, n_clusters=3, compute_labels=True, copy=True)\n",
        "```\n",
        "\n",
        "**Fator de ramificação (branching factor)**: Este parâmetro controla quantos subclusters podem ser mesclados em um único cluster durante a fase de fusão de subclusters. Um fator de ramificação maior pode resultar em árvores mais profundas e mais amplas. Um valor padrão comum é 50.\n",
        "\n",
        "**Limiar de espalhamento (threshold)**: O limiar de espalhamento é usado para determinar quando um novo subcluster deve ser criado. Ele controla a quantidade de dispersão dentro de um subcluster. Pontos dentro do mesmo subcluster devem estar dentro de uma certa distância do centroide do subcluster. Se a distância exceder o limiar de espalhamento, um novo subcluster é criado. O valor padrão pode variar, mas geralmente é definido empiricamente com base na natureza dos dados.\n",
        "\n",
        "**Número máximo de subclusters (maximum number of subclusters)**: Este parâmetro define o número máximo de subclusters que podem ser mantidos em cada nó da árvore. Se esse número for excedido, os subclusters são mesclados para manter a complexidade da árvore sob controle.\n",
        "\n",
        "**Altura máxima da árvore (maximum tree height)**: Define a altura máxima permitida para a árvore CF. Se a altura máxima for atingida durante a construção da árvore, os subclusters serão mesclados para manter a altura da árvore dentro do limite.\n",
        "\n",
        "**Método de mesclagem de subclusters (subcluster merging method)**: Este parâmetro define como os subclusters devem ser mesclados durante a fusão de subclusters. Existem diferentes métodos que podem ser utilizados, como mesclagem baseada em distância ou mesclagem baseada em densidade."
      ],
      "metadata": {
        "id": "xiWB4ux9m3WT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code"
      ],
      "metadata": {
        "id": "k1ATO7OfzocI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Melhores metricas"
      ],
      "metadata": {
        "id": "YT2ZE57EPsQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import Birch\n",
        "from sklearn.metrics import calinski_harabasz_score\n",
        "\n",
        "def treina_birch(X, threshold_values, branching_factor_values):\n",
        "    best_model = None\n",
        "    best_cohesion = float('inf')\n",
        "    best_separation = float('inf')\n",
        "    best_n_clusters = -1\n",
        "    best_threshold = None\n",
        "    best_branching_factor = None\n",
        "\n",
        "    for threshold in threshold_values:\n",
        "        for branching_factor in branching_factor_values:\n",
        "            model = Birch(threshold=threshold, branching_factor=branching_factor)\n",
        "            model.fit(X)\n",
        "            labels = model.predict(X)\n",
        "            cohesion = calinski_harabasz_score(X, labels)\n",
        "\n",
        "            # Calcula a separação\n",
        "            centroids = model.subcluster_centers_\n",
        "            separation = 0\n",
        "            for i in range(len(centroids)):\n",
        "                for j in range(i + 1, len(centroids)):\n",
        "                    separation += np.linalg.norm(centroids[i] - centroids[j])\n",
        "            separation /= (len(centroids) * (len(centroids) - 1) / 2)\n",
        "\n",
        "            n_clusters = len(model.subcluster_centers_)\n",
        "            if cohesion < best_cohesion:\n",
        "                best_model = model\n",
        "                best_cohesion = cohesion\n",
        "                best_separation = separation\n",
        "                best_n_clusters = n_clusters\n",
        "                best_branching_factor = branching_factor\n",
        "\n",
        "    return best_model, best_cohesion, best_separation, best_n_clusters, best_branching_factor\n"
      ],
      "metadata": {
        "id": "SC4OCe7cIra2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir listas de valores para threshold e branching factor\n",
        "threshold_values = [0.1, 0.5, 1.0,2.0]\n",
        "branching_factor_values = [50, 100, 200,400]\n",
        "\n",
        "best_model, best_cohesion, best_separation, best_n_clusters, best_branching_factor = treina_birch(Ctreino, threshold_values, branching_factor_values)\n",
        "\n",
        "print(f'branching factor: {best_branching_factor}')\n",
        "print(f'melhor modelo: {best_model}')\n",
        "print(f'Número de clusters: {best_n_clusters}')\n",
        "print(f'Coesão: {best_cohesion}')\n",
        "print(f'Separation: {best_separation}')"
      ],
      "metadata": {
        "id": "NbG5ErhSItnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def treina_birch(Ctreino, n_clusters):\n",
        "    for _ in range(10):\n",
        "        birch = Birch(n_clusters=n_clusters)\n",
        "        birch.fit(Ctreino)\n",
        "    return birch\n",
        "\n",
        "# Crie uma lista para armazenar os valores de inércia\n",
        "silhouettes = []\n",
        "n = 20\n",
        "\n",
        "# Execute o BIRCH para diferentes valores de clusters\n",
        "for k in tqdm(range(2, n)):\n",
        "    birch = treina_birch(Ctreino, k)\n",
        "    labels = birch.predict(Cvad)\n",
        "    silhouette_avg = silhouette_score(Cvad, labels)\n",
        "    silhouettes.append(silhouette_avg)"
      ],
      "metadata": {
        "id": "jLetvLORMEw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trace o gráfico da silhueta\n",
        "plt.plot(silhouettes, marker='o')\n",
        "plt.xlabel('Número de Clusters (k)-2')\n",
        "plt.xticks(range(len(silhouettes)),range(len(silhouettes)) )\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Avaliação do Número de Clusters ')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Jf0jX_tqMI9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Clusterizar"
      ],
      "metadata": {
        "id": "xsHOw_H5Pyc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import Birch\n",
        "\n",
        "n_clusters = 9\n",
        "\n",
        "# Crie o objeto de clusterização Birch\n",
        "birch = Birch(n_clusters=n_clusters)\n",
        "\n",
        "# Realize a clusterização\n",
        "clusters = birch.fit_predict(Ctreino)\n",
        "\n",
        "# Adicione os clusters de volta ao DataFrame original\n",
        "Ctreino2 = Ctreino.copy()\n",
        "Ctreino2['Cluster'] = clusters\n",
        "Ctreino2"
      ],
      "metadata": {
        "id": "yP2XFQO-QKIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Novos Dados"
      ],
      "metadata": {
        "id": "y76TTM8nP-7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clusterizar(data,medtrain,stdtrain,birch):\n",
        "  df1 = data\n",
        "  df1 = (df1 - medtrain)/ stdtrain\n",
        "  labels = birch.predict(df1)\n",
        "  return labels"
      ],
      "metadata": {
        "id": "D2R4pKVDSi0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = []\n",
        "for index, row in pessoasnovas.iterrows():\n",
        "  labels.append(clusterizar(pessoasnovas.iloc[[index]],medtrain,stdtrain,birch)[0])\n",
        "\n",
        "pessoasnovas2 = pessoasnovas.copy()\n",
        "pessoasnovas2['Cluster'] = labels\n",
        "\n",
        "pessoasnovas2"
      ],
      "metadata": {
        "id": "yKmLqAbGStW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DBSCAN"
      ],
      "metadata": {
        "id": "rCWCZPheQPSZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explicação"
      ],
      "metadata": {
        "id": "GXW4WJzNzt5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O algoritmo DBSCAN (Density-Based Spatial Clustering of Applications with Noise) é uma técnica de clusterização que agrupa pontos de dados com base na densidade local.\n",
        "\n",
        "1. **Definição de parâmetros**: O DBSCAN requer dois parâmetros principais:\n",
        "   - **Eps (ε)**: É a distância máxima que define a vizinhança de um ponto.\n",
        "   - **MinPts**: É o número mínimo de pontos que devem estar dentro da vizinhança de um ponto para que ele seja considerado um \"ponto central\".\n",
        "\n",
        "2. **Identificação de pontos centrais (Core) e pontos de fronteira (Boundary)**: O algoritmo começa selecionando aleatoriamente um ponto de dados e verifica se há pelo menos MinPts pontos dentro de sua vizinhança (incluindo ele mesmo). Se houver, o ponto é marcado como um \"ponto central\". Se não, é marcado como um \"ponto de fronteira\".\n",
        "\n",
        "<img src=\"https://www.researchgate.net/profile/Yijin-Liu-2/publication/308750501/figure/fig4/AS:412083041652736@1475259661770/Schematic-drawings-of-the-DBSCAN-clustering-algorithm-Panel-a-shows-the-clustering.png\" alt=\"Imagem\" width=\"800\" height=\"600\">\n",
        "\n",
        "3. **Expansão de clusters**: Para cada \"ponto central\", o algoritmo expande o cluster incluindo todos os pontos atingíveis dentro de uma distância ε a partir desse ponto. Isso significa que o algoritmo verifica se os pontos dentro da vizinhança de cada \"ponto central\" também são \"pontos centrais\" e, se forem, expande o cluster para incluí-los também.\n",
        "\n",
        "4. **Identificação de ruído (outlier)**: Os pontos que não são \"pontos centrais\" nem \"pontos de fronteira\" são considerados ruído e não são atribuídos a nenhum cluster.\n",
        "\n",
        "5. **Resultados**: O algoritmo termina quando todos os pontos foram processados. Os clusters são formados pelos pontos que foram alcançáveis a partir dos \"pontos centrais\", e os \"pontos de fronteira\" são atribuídos aos clusters apropriados. Os pontos que não pertencem a nenhum cluster são considerados ruído.\n",
        "\n",
        "O DBSCAN é eficaz para identificar clusters de diferentes formas e tamanhos, além de poder lidar com ruído e outliers nos dados. No entanto, é sensível à escolha dos parâmetros ε e MinPts, e encontrar valores adequados para esses parâmetros pode exigir um certo grau de experimentação e entendimento dos dados.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "class sklearn.cluster.DBSCAN(eps=0.5, *, min_samples=5, metric='euclidean', metric_params=None, algorithm='auto', leaf_size=30, p=None, n_jobs=None)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "1v3L2noOztv-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code"
      ],
      "metadata": {
        "id": "MqJiE1fhztkQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Melhores metricas"
      ],
      "metadata": {
        "id": "KkKV7GCeQwlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "def calculate_separability(X, labels):\n",
        "    if len(np.unique(labels)) == 1:\n",
        "        return 0.0\n",
        "    nbrs = NearestNeighbors(n_neighbors=2).fit(X)\n",
        "    distances, indices = nbrs.kneighbors(X)\n",
        "    separability = np.mean(distances[:, 1])\n",
        "    return separability\n",
        "\n",
        "def treina_dbscan(X, eps_values, min_samples_values):\n",
        "    best_model = None\n",
        "    best_silhouette = -1\n",
        "    best_separability = -1\n",
        "    best_avg_cluster_size = -1\n",
        "    best_eps = None\n",
        "    best_min_samples = None\n",
        "\n",
        "    for eps in eps_values:\n",
        "        for min_samples in min_samples_values:\n",
        "            model = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "            labels = model.fit_predict(X)\n",
        "\n",
        "            # Check if there's only one cluster\n",
        "            if len(np.unique(labels)) == 1:\n",
        "                continue\n",
        "\n",
        "            silhouette = silhouette_score(X, labels)\n",
        "            separability = calculate_separability(X, labels)\n",
        "            unique, counts = np.unique(labels, return_counts=True)\n",
        "            avg_cluster_size = np.mean(counts)\n",
        "\n",
        "            if silhouette > best_silhouette:\n",
        "                best_model = model\n",
        "                best_silhouette = silhouette\n",
        "                best_separability = separability\n",
        "                best_avg_cluster_size = avg_cluster_size\n",
        "                best_eps = eps\n",
        "                best_min_samples = min_samples\n",
        "\n",
        "    return best_model, best_silhouette, best_separability, best_avg_cluster_size, best_eps, best_min_samples\n"
      ],
      "metadata": {
        "id": "n4pGv8yLKzVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defina uma lista de valores para eps e min_samples\n",
        "eps_values = [0.1, 0.5, 1.0]\n",
        "min_samples_values = [5, 10, 15]\n",
        "\n",
        "best_model, best_silhouette, best_separability, best_avg_cluster_size, best_eps, best_min_samples = treina_dbscan(Ctreino, eps_values, min_samples_values)\n",
        "\n",
        "print(f'Silhueta: {best_silhouette}')\n",
        "print(f'Separabilidade: {best_separability}')\n",
        "print(f'Tamanho médio dos clusters: {best_avg_cluster_size}')\n",
        "print(f'Valor de eps utilizado: {best_eps}')\n",
        "print(f'Valor de min_samples utilizado: {best_min_samples}')\n"
      ],
      "metadata": {
        "id": "Pgtsl4M-K20G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Clusterizando"
      ],
      "metadata": {
        "id": "Hn2dM02mQ0SI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "def treina_dbscan(Ctreino, eps, min_samples):\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "    dbscan.fit(Ctreino)\n",
        "    return dbscan\n",
        "\n",
        "# Lista para armazenar os valores de silhueta\n",
        "silhouettes = []\n",
        "\n",
        "# Defina os valores mínimos e máximos para eps e min_samples\n",
        "eps_min = 0.1\n",
        "eps_max = 2.0\n",
        "min_samples_min = 5\n",
        "min_samples_max = 20\n",
        "\n",
        "# Número de valores que você deseja gerar dentro do intervalo\n",
        "num_eps = 10\n",
        "num_samples = 15\n",
        "\n",
        "# Gere os valores para eps e min_samples\n",
        "eps_values = np.linspace(eps_min, eps_max, num=num_eps)\n",
        "min_samples_values = np.linspace(min_samples_min, min_samples_max, num=num_samples, dtype=int)\n",
        "\n",
        "# Loop para testar diferentes combinações de eps e min_samples\n",
        "for eps in tqdm(eps_values):\n",
        "    for min_samples in min_samples_values:\n",
        "        # Treina o modelo DBSCAN com os parâmetros atuais\n",
        "        dbscan = treina_dbscan(Ctreino, eps, min_samples)\n",
        "\n",
        "        # Obtém os rótulos dos clusters\n",
        "        labels = dbscan.labels_\n",
        "\n",
        "        # Verifica se há mais de um rótulo além do rótulo de ruído (-1)\n",
        "        unique_labels = set(labels)\n",
        "        n_clusters = len(unique_labels) - (1 if -1 in labels else 0)\n",
        "\n",
        "        # Se houver mais de um cluster (excluindo o rótulo de ruído), calcule a silhueta\n",
        "        if n_clusters > 1:\n",
        "            silhouette_avg = silhouette_score(Ctreino, labels)\n",
        "            silhouettes.append((eps, min_samples, silhouette_avg))\n",
        "\n",
        "# Encontre os parâmetros que produzem a maior pontuação de silhueta\n",
        "best_eps, best_min_samples, best_silhouette = max(silhouettes, key=lambda x: x[2])\n",
        "\n",
        "print(f\"Melhor configuração: eps={best_eps}, min_samples={best_min_samples}, silhouette_score={best_silhouette}, n_cluster={n_clusters}\")"
      ],
      "metadata": {
        "id": "7k6hbNpOQf-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eps= 1.4\n",
        "min_samples= 5\n",
        "\n",
        "# Criando o objeto de clusterização DBSCAN\n",
        "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "\n",
        "# Realizando a clusterização\n",
        "clusters = dbscan.fit_predict(Ctreino)\n",
        "\n",
        "# Adicionando os clusters de volta ao DataFrame original\n",
        "Ctreino2 = Ctreino.copy()\n",
        "Ctreino2['Cluster'] = clusters"
      ],
      "metadata": {
        "id": "uTHFUOu7ad2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Novos Dados"
      ],
      "metadata": {
        "id": "d3Fnx3byQ8QA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clusterizar(data,medtrain,stdtrain,dbscan):\n",
        "  df1 = data\n",
        "  df1 = (df1 - medtrain)/ stdtrain\n",
        "  labels = dbscan.fit_predict(df1)\n",
        "  return labels"
      ],
      "metadata": {
        "id": "oOdthq_Ebt-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = []\n",
        "for index, row in pessoasnovas.iterrows():\n",
        "  labels.append(clusterizar(pessoasnovas.iloc[[index]],medtrain,stdtrain,dbscan)[0])\n",
        "\n",
        "pessoasnovas2 = pessoasnovas.copy()\n",
        "pessoasnovas2['Cluster'] = labels\n",
        "\n",
        "pessoasnovas2"
      ],
      "metadata": {
        "id": "MKqGbLwCa8F0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mean Shift"
      ],
      "metadata": {
        "id": "v-2HdpWyQPLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explicação"
      ],
      "metadata": {
        "id": "SWV_3Tn-zzyb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O algoritmo Mean Shift é uma técnica de clusterização que não requer a especificação do número de clusters a priori.\n",
        "\n",
        "1. **Seleção dos centros iniciais**: O algoritmo começa com uma etapa de inicialização, onde os \"centros\" dos clusters são selecionados. Isso pode ser feito escolhendo aleatoriamente pontos no conjunto de dados ou usando uma estratégia mais sistemática.\n",
        "\n",
        "2. **Deslocamento médio (Mean Shift)**: Para cada centro inicial, o algoritmo calcula o deslocamento médio ponderado dos pontos de dados em direção a regiões de maior densidade de pontos. Esse deslocamento é calculado como a média ponderada dos vetores de deslocamento de todos os pontos em relação ao centro.\n",
        "\n",
        "3. **Atualização dos centros**: Os centros dos clusters são atualizados movendo-os na direção do deslocamento médio calculado na etapa anterior.\n",
        "\n",
        "4. **Convergência**: Os passos 2 e 3 são repetidos até que os centros dos clusters não mudem significativamente entre as iterações ou até que um critério de parada seja alcançado. Geralmente, um critério de parada pode ser um número máximo de iterações ou quando a mudança nos centros dos clusters é pequena o suficiente.\n",
        "\n",
        "5. **Atribuição de pontos aos clusters**: Depois que os centros dos clusters convergem, os pontos de dados são atribuídos aos clusters com base na proximidade com os centros. Cada ponto é atribuído ao cluster cujo centro está mais próximo.\n",
        "\n",
        "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20190429213154/1354.png\" alt=\"Imagem\" width=\"500\" height=\"400\">\n",
        "\n",
        "O algoritmo Mean Shift é eficaz para identificar clusters com diferentes formas e tamanhos, e pode encontrar clusters de densidade variável nos dados. Ele é particularmente útil quando a estrutura dos dados não é conhecida a priori e quando os clusters têm formas irregulares ou distribuições de densidade variáveis."
      ],
      "metadata": {
        "id": "eIdFCmv0zzu8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parametros"
      ],
      "metadata": {
        "id": "PD0bNjdWr6a_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "sklearn.cluster.estimate_bandwidth(X, *, quantile=0.3, n_samples=None, random_state=0, n_jobs=None)\n",
        "```\n",
        "\n",
        "\n",
        "1. **Banda (Bandwidth)**: Este é o parâmetro mais importante do algoritmo Mean Shift. A banda (bandwidth) define a escala espacial sobre a qual a densidade dos dados é avaliada. Ela determina o tamanho da janela de busca ao redor de cada ponto para calcular o deslocamento médio. Uma banda maior resulta em uma janela de busca maior, o que pode levar a uma suavização maior dos clusters, enquanto uma banda menor pode resultar em uma identificação de clusters mais detalhada, mas também pode ser mais sensível a ruídos.\n",
        "\n",
        "Além do parâmetro de banda, existem outros parâmetros secundários que podem ser ajustados dependendo da implementação específica do algoritmo Mean Shift:\n",
        "\n",
        "```\n",
        "class sklearn.cluster.MeanShift(*, bandwidth=None, seeds=None, bin_seeding=False, min_bin_freq=1, cluster_all=True, n_jobs=None, max_iter=300)\n",
        "```\n",
        "\n",
        "2. **Kernel**: O algoritmo Mean Shift utiliza um kernel para ponderar a contribuição de cada ponto no cálculo do deslocamento médio. O tipo de kernel e sua largura podem afetar significativamente os resultados da clusterização.\n",
        "\n",
        "3. **Método de convergência**: O algoritmo pode ter critérios diferentes para determinar a convergência, como um número máximo de iterações ou uma tolerância para a mudança nos centros dos clusters.\n",
        "\n",
        "4. **Inicialização dos centros**: A maneira como os centros dos clusters são inicializados também pode influenciar o desempenho do algoritmo. Isso pode incluir estratégias como escolha aleatória de pontos como centros iniciais ou seleção de pontos baseados em critérios específicos.\n",
        "\n",
        "A escolha dos parâmetros adequados é crucial para obter bons resultados de clusterização com o algoritmo Mean Shift. Experimentação e ajuste dos parâmetros com base nas características dos seus dados e nos objetivos da análise são essenciais para obter resultados significativos e úteis."
      ],
      "metadata": {
        "id": "npcfenjMr8I9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code"
      ],
      "metadata": {
        "id": "rZ61JWKgzznU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Melhores parametros"
      ],
      "metadata": {
        "id": "P0K_nNKpT30P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.metrics.pairwise import pairwise_distances\n",
        "\n",
        "def calculate_separability(cluster_centers):\n",
        "    if len(cluster_centers) <= 1:\n",
        "        return 0.0\n",
        "    return np.mean(pairwise_distances(cluster_centers, metric='euclidean'))\n",
        "\n",
        "def calculate_cohesion(X, labels, cluster_centers):\n",
        "    cohesion = 0.0\n",
        "    for label in np.unique(labels):\n",
        "        cluster_points = X[labels == label]\n",
        "        if len(cluster_points) > 0:\n",
        "            cluster_cohesion = np.mean(np.sum((cluster_points - cluster_centers[label])**2, axis=1))\n",
        "            cohesion += cluster_cohesion\n",
        "    return cohesion / len(X)\n",
        "\n",
        "def treina_ms(Ctreino, quantile, samples):\n",
        "    bandwidth = estimate_bandwidth(Ctreino, quantile=quantile, n_samples=samples)\n",
        "    ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
        "    ms.fit(Ctreino)\n",
        "\n",
        "    labels = ms.labels_\n",
        "    cluster_centers = ms.cluster_centers_\n",
        "    n_clusters_ = len(np.unique(labels))\n",
        "\n",
        "    if n_clusters_ > 1:\n",
        "        silhouette = silhouette_score(Ctreino, labels)\n",
        "        separability = calculate_separability(cluster_centers)\n",
        "        cohesion = calculate_cohesion(Ctreino, labels, cluster_centers)\n",
        "    else:\n",
        "        silhouette = -1\n",
        "        separability = -1\n",
        "        cohesion = float('inf')\n",
        "\n",
        "    return ms, silhouette, separability, cohesion, n_clusters_"
      ],
      "metadata": {
        "id": "PQBeFHZjLmm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_quantile = 0.1\n",
        "max_quantile = 1.0\n",
        "quantile_values = np.linspace(min_quantile, max_quantile, num=Div_quantile)\n",
        "\n",
        "min_samples = 20\n",
        "max_samples = n_samples\n",
        "n_samples_values = np.linspace(min_samples, max_samples, num=Div_sample, dtype=int)\n",
        "\n",
        "\n",
        "best_silhouette = -1\n",
        "best_separability = -1\n",
        "best_cohesion = float('inf')\n",
        "best_quantile = None\n",
        "best_samples = None\n",
        "best_model = None\n",
        "\n",
        "for quantile in tqdm(quantile_values):\n",
        "    for samples in n_samples_values:\n",
        "        ms, silhouette, separability, cohesion, n_clusters_ = treina_ms(Ctreino, quantile, samples)\n",
        "        if silhouette > best_silhouette and separability > best_separability and cohesion < best_cohesion:\n",
        "            best_silhouette = silhouette\n",
        "            best_separability = separability\n",
        "            best_cohesion = cohesion\n",
        "            best_quantile = quantile\n",
        "            best_samples = samples\n",
        "            best_model = ms\n",
        "\n",
        "print('Melhores parâmetros encontrados:')\n",
        "print('Quantile:', best_quantile)\n",
        "print('Samples:', best_samples)\n",
        "print(\"Número estimado de clusters:\", len(np.unique(best_model.labels_)))\n",
        "print('Silhueta:', best_silhouette)\n",
        "print('Separabilidade:', best_separability)\n",
        "print('Coesão:', best_cohesion)"
      ],
      "metadata": {
        "id": "WZcGa1nISpkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Clusterizando"
      ],
      "metadata": {
        "id": "Wt18VMomT0_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
        "\n",
        "# Estime a largura de banda\n",
        "bandwidth = estimate_bandwidth(Ctreino, quantile=0.1, n_samples=20)\n",
        "\n",
        "# Crie o objeto de clusterização Mean Shift\n",
        "ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
        "\n",
        "# Realize a clusterização\n",
        "ms.fit(Ctreino)\n",
        "\n",
        "# Obtenha os rótulos dos clusters e os centros dos clusters\n",
        "labels = ms.labels_\n",
        "\n",
        "# Realizando a clusterização\n",
        "clusters = ms.fit_predict(Ctreino)\n",
        "\n",
        "# Adicionando os clusters de volta ao DataFrame original\n",
        "Ctreino2 = Ctreino.copy()\n",
        "Ctreino2['Cluster'] = clusters"
      ],
      "metadata": {
        "id": "KwixJPtCJjCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ctreino2['Cluster'].value_counts()"
      ],
      "metadata": {
        "id": "z1qMdulIWeI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OPTICS"
      ],
      "metadata": {
        "id": "ViY8dxdnQPEM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explicação"
      ],
      "metadata": {
        "id": "4DLalihXz7Dz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O algoritmo OPTICS (Ordering Points To Identify the Clustering Structure) é uma técnica de clusterização que se baseia na densidade dos pontos de dados.\n",
        "\n",
        "1. **Cálculo da distância**: O algoritmo começa calculando a distância entre todos os pares de pontos no conjunto de dados.\n",
        "\n",
        "2. **Ordenação dos pontos**: Os pontos de dados são então ordenados com base em sua densidade. Isso é feito determinando a distância até o ponto mais próximo que tenha uma densidade maior ou igual a um certo limiar (MinPts), que é um parâmetro definido pelo usuário.\n",
        "\n",
        "3. **Construção da estrutura OPTICS**: Durante a ordenação, o algoritmo constrói uma estrutura de árvore chamada de OPTICS Reachability Plot. Esta estrutura é semelhante a um dendrograma e representa a hierarquia da densidade dos pontos de dados.\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*HRwa9lQgPhwoCAVlX4S_xA.png\" alt=\"Imagem\" width=\"800\" height=\"400\">\n",
        "\n",
        "4. **Identificação de clusters**: Após a construção da estrutura OPTICS, é possível identificar os clusters observando os \"vales\" na estrutura. Um vale indica uma região de baixa densidade, que pode ser interpretada como uma fronteira entre clusters.\n",
        "\n",
        "5. **Extração dos clusters**: Os clusters são extraídos da estrutura OPTICS considerando os vales e as densidades dos pontos. Isso permite identificar clusters de diferentes formas e tamanhos, além de lidar com ruído e outliers de forma mais eficaz.\n",
        "\n",
        "O algoritmo OPTICS é eficaz para identificar clusters em conjuntos de dados com densidade variável e pode lidar com ruído e outliers de forma robusta. Ele fornece uma visão hierárquica da estrutura de clusterização dos dados, o que pode ser útil para análises exploratórias e identificação de padrões complexos."
      ],
      "metadata": {
        "id": "V3xX-9Cez7AX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parametros"
      ],
      "metadata": {
        "id": "B7YAeo5mwTkq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "class sklearn.cluster.OPTICS(*, min_samples=5, max_eps=inf, metric='minkowski', p=2, metric_params=None, cluster_method='xi', eps=None, xi=0.05, predecessor_correction=True, min_cluster_size=None, algorithm='auto', leaf_size=30, memory=None, n_jobs=None)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1. **MinPts**: Este é o parâmetro mais importante do algoritmo OPTICS. MinPts define o número mínimo de pontos que devem existir dentro de uma vizinhança de um ponto para que ele seja considerado parte de um cluster. Pontos que têm menos vizinhos do que MinPts são considerados ruído. Escolher um valor adequado para MinPts é crucial para identificar clusters significativos e evitar a identificação de pequenos grupos espúrios.\n",
        "\n",
        "2. **Eps**: O parâmetro Eps (ε) define a distância máxima entre dois pontos para que sejam considerados vizinhos. É usado para calcular a densidade dos pontos em torno de cada ponto e para identificar a estrutura de densidade dos dados. Escolher um valor apropriado para Eps depende da escala dos dados e da densidade esperada dos clusters.\n",
        "\n",
        "3. **Algoritmo de vizinhança**: O algoritmo OPTICS pode usar diferentes algoritmos para calcular vizinhos próximos, como k-d trees ou algoritmos de busca espacial. A escolha do algoritmo de vizinhança pode afetar significativamente o desempenho do algoritmo e a eficiência do cálculo de vizinhança em grandes conjuntos de dados.\n",
        "\n",
        "4. **Método de distância**: O algoritmo OPTICS pode usar diferentes métodos de cálculo de distância, como distância euclidiana, distância de Manhattan, distância de Minkowski, entre outros. A escolha do método de distância depende das características dos dados e da natureza do problema de clusterização."
      ],
      "metadata": {
        "id": "rfhVVPiJwUTb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code"
      ],
      "metadata": {
        "id": "dnDuybOxz69t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import OPTICS\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.metrics.pairwise import pairwise_distances\n",
        "import numpy as np\n",
        "\n",
        "def calculate_separability(cluster_labels, X):\n",
        "    unique_labels = np.unique(cluster_labels)\n",
        "    separability = 0.0\n",
        "    for label in unique_labels:\n",
        "        cluster_points = X[cluster_labels == label]\n",
        "        centroid = np.mean(cluster_points, axis=0)\n",
        "        separability += np.sum(pairwise_distances(cluster_points, [centroid]))\n",
        "    return separability\n",
        "\n",
        "def evaluate_optics(X, min_samples_range, max_eps_range):\n",
        "    best_silhouette = -1\n",
        "    best_separability = -1\n",
        "    best_structure = -1\n",
        "    best_min_samples = None\n",
        "    best_max_eps = None\n",
        "    best_n_clusters = -1  # Variável para armazenar o número de clusters\n",
        "\n",
        "    min_samples_values = np.linspace(min_samples_range[0], min_samples_range[1], num=20)\n",
        "    max_eps_values = np.linspace(max_eps_range[0], max_eps_range[1], num=20)\n",
        "\n",
        "    pbar = tqdm(total=len(min_samples_values) * len(max_eps_values))\n",
        "\n",
        "    for min_samples in min_samples_values:\n",
        "        for max_eps in max_eps_values:\n",
        "            optics = OPTICS(min_samples=int(min_samples), max_eps=max_eps)\n",
        "            try:\n",
        "                optics.fit(X)\n",
        "            except ValueError:\n",
        "                continue\n",
        "            labels = optics.labels_  # Rótulos gerados pelo OPTICS\n",
        "            n_clusters = len(np.unique(labels))  # Contagem de rótulos únicos\n",
        "            if n_clusters > 1:  # Consideramos apenas configurações válidas com mais de um cluster\n",
        "                silhouette = silhouette_score(X, labels)\n",
        "                separability = calculate_separability(labels, X)\n",
        "                structure = n_clusters\n",
        "                if silhouette > best_silhouette:\n",
        "                    best_silhouette = silhouette\n",
        "                    best_separability = separability\n",
        "                    best_structure = structure\n",
        "                    best_min_samples = min_samples\n",
        "                    best_max_eps = max_eps\n",
        "                    best_n_clusters = n_clusters\n",
        "            pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    return best_silhouette, best_separability, best_structure, best_min_samples, best_max_eps, best_n_clusters"
      ],
      "metadata": {
        "id": "wZTCDLDiY15b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defina os intervalos para os parâmetros min_samples e max_eps\n",
        "min_samples_range = (5, 50)  # Amplie o intervalo conforme necessário\n",
        "max_eps_range = (2,10)  # Amplie o intervalo conforme necessário\n",
        "\n",
        "# Avalie o OPTICS com diferentes combinações de parâmetros\n",
        "best_silhouette, best_separability, best_structure, best_min_samples, best_max_eps, best_n_clusters = evaluate_optics(Ctreino, min_samples_range, max_eps_range)\n",
        "\n",
        "# Apresente os melhores resultados encontrados\n",
        "if best_min_samples is not None and best_max_eps is not None:\n",
        "    print('Melhores parâmetros encontrados:')\n",
        "    print('Min_samples:', best_min_samples)\n",
        "    print('Max_eps:', best_max_eps)\n",
        "    print('Silhueta:', best_silhouette)\n",
        "    print('Separabilidade:', best_separability)\n",
        "    print('Estrutura do Cluster:', best_structure)\n",
        "    print('Número de Clusters:', best_n_clusters)\n",
        "else:\n",
        "    print('Nenhum conjunto de parâmetros válido encontrado.')"
      ],
      "metadata": {
        "id": "h0CmvCcmY3jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import OPTICS\n",
        "\n",
        "# Crie o objeto OPTICS\n",
        "optics_model = OPTICS(min_samples=2, xi=.05, min_cluster_size=.05)\n",
        "\n",
        "# Ajuste o modelo aos dados\n",
        "optics_model.fit(Ctreino)\n",
        "\n",
        "# Atribuições de cluster\n",
        "clusters = optics_model.labels_\n",
        "\n",
        "# Adicionando os clusters de volta ao DataFrame original\n",
        "Ctreino2 = Ctreino.copy()\n",
        "Ctreino2['Cluster'] = clusters"
      ],
      "metadata": {
        "id": "xsHg5TQ-QiZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ctreino2['Cluster'].value_counts()"
      ],
      "metadata": {
        "id": "uG3bLLSHnhaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spectral Clustering"
      ],
      "metadata": {
        "id": "ZrNiUFLRQOzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explicação"
      ],
      "metadata": {
        "id": "lXEG18Gmz_a_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O método Spectral Clustering é uma técnica de clusterização que utiliza propriedades dos autovetores de matrizes para encontrar estruturas de clusters nos dados.\n",
        "\n",
        "1. **Construção da matriz de similaridade**: O primeiro passo é construir uma matriz de similaridade que capture as relações entre os pontos de dados. Esta matriz pode ser construída usando diferentes medidas de similaridade, como distância euclidiana, distância de similaridade ou outras métricas relevantes para o problema.\n",
        "\n",
        "2. **Construção da matriz laplaciana**: A matriz laplaciana é derivada da matriz de similaridade e é usada para capturar a estrutura de conectividade dos dados. Existem diferentes formas de construir a matriz laplaciana, sendo a mais comum a matriz laplaciana não normalizada ou a matriz laplaciana normalizada.\n",
        "\n",
        "3. **Cálculo dos autovetores**: Os autovetores (e possivelmente autovalores) da matriz laplaciana são calculados. Os autovetores correspondentes aos menores autovalores capturam a estrutura de cluster dos dados.\n",
        "\n",
        "4. **Redução de dimensionalidade e agrupamento espectral**: Os autovetores são usados para reduzir a dimensionalidade dos dados para um espaço de menor dimensão, onde é mais fácil identificar os clusters. Em seguida, um algoritmo de agrupamento, como k-means, é aplicado neste espaço de menor dimensão para atribuir os pontos aos clusters.\n",
        "\n",
        "5. **Atribuição de pontos aos clusters**: Finalmente, os pontos de dados são atribuídos aos clusters identificados pelo algoritmo de agrupamento no espaço de menor dimensão.\n",
        "\n",
        "O método Spectral Clustering é eficaz para identificar clusters de formas complexas e não lineares nos dados. Ele é frequentemente utilizado em problemas de clusterização onde os clusters não são linearmente separáveis e apresentam estruturas intrínsecas mais complexas.\n",
        "\n",
        "\n",
        "<img src=\"https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-65347-7_8/MediaObjects/498806_1_En_8_Fig1_HTML.png\" alt=\"Imagem\" width=\"900\" height=\"300\">\n"
      ],
      "metadata": {
        "id": "XNodYvI1z_Xp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parametros"
      ],
      "metadata": {
        "id": "VE4RVOgFzqiG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "class sklearn.cluster.SpectralClustering(n_clusters=8, *, eigen_solver=None, n_components=None, random_state=None, n_init=10, gamma=1.0, affinity='rbf', n_neighbors=10, eigen_tol='auto', assign_labels='kmeans', degree=3, coef0=1, kernel_params=None, n_jobs=None, verbose=False)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "1. **Número de Clusters (k)**: Este é o número de grupos nos quais você deseja segmentar seus dados. Escolher o número correto de clusters é crucial e pode exigir alguma investigação e compreensão do conjunto de dados.\n",
        "\n",
        "2. **Matriz de Similaridade (ou afinidade)**: Esta é a matriz que descreve a relação de proximidade ou similaridade entre as amostras do conjunto de dados. As opções comuns incluem a matriz de adjacência, matriz de distância euclidiana, matriz de similaridade baseada em grafos, entre outras.\n",
        "\n",
        "3. **Tipo de Kernel (se aplicável)**: Se você estiver usando uma matriz de similaridade baseada em kernel (por exemplo, o kernel de Gaussiano), precisará especificar o tipo de kernel e seus parâmetros, como a largura de banda para o kernel de Gaussiano.\n",
        "\n",
        "4. **Método de Decomposição Espectral**: A matriz de similaridade é transformada em uma matriz Laplaciana, que é então decomposta em seus componentes principais. Existem diferentes métodos para realizar essa decomposição, como a Decomposição em Valores Singulares (SVD), Decomposição Espectral Normalizada, entre outros.\n",
        "\n",
        "5. **Método de Atribuição de Cluster**: Depois de obter os componentes principais da matriz Laplaciana, é necessário atribuir as amostras aos clusters. Isso pode ser feito de várias maneiras, como o método k-means aplicado aos componentes principais ou métodos de corte espectral.\n",
        "\n",
        "6. **Parâmetros Específicos do Método de Atribuição de Cluster**: Dependendo do método de atribuição de cluster escolhido, pode haver parâmetros adicionais a serem ajustados, como o número máximo de iterações no k-means."
      ],
      "metadata": {
        "id": "yJmTbufhzsjx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code"
      ],
      "metadata": {
        "id": "A-uF_HzMz_Uu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import SpectralClustering\n",
        "\n",
        "# Criar o objeto SpectralClustering\n",
        "spectral_model = SpectralClustering(n_clusters=6, affinity='nearest_neighbors')\n",
        "\n",
        "# Ajustar o modelo aos dados\n",
        "spectral_model.fit(Ctreino)\n",
        "\n",
        "# Atribuições de cluster\n",
        "clusters = spectral_model.labels_\n",
        "\n",
        "# Adicionando os clusters de volta ao DataFrame original\n",
        "Ctreino2 = Ctreino.copy()\n",
        "Ctreino2['Cluster'] = clusters"
      ],
      "metadata": {
        "id": "IrdDMEM-Qn-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ctreino2['Cluster'].value_counts()"
      ],
      "metadata": {
        "id": "NmY2bUBvpv_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gaussian Mixture Model"
      ],
      "metadata": {
        "id": "9LCISweCQoLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explicação"
      ],
      "metadata": {
        "id": "lXMmc6ED0Ef0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Claro! O método Gaussian Mixture Model (GMM) é uma técnica de agrupamento probabilístico que assume que os dados são gerados por uma mistura de várias distribuições gaussianas.\n",
        "\n",
        "1. **Inicialização**: Primeiro, você precisa inicializar os parâmetros do modelo, incluindo o número de clusters e os parâmetros das distribuições gaussianas para cada cluster. Isso pode ser feito de várias maneiras, como aleatoriamente ou usando algum método de inicialização.\n",
        "\n",
        "2. **Expectation-Maximization (EM)**: Depois da inicialização, o algoritmo EM é utilizado para ajustar os parâmetros do modelo iterativamente. Este algoritmo tem duas etapas:\n",
        "\n",
        "   - **Expectation Step (Passo de Expectativa)**: Nesta etapa, o algoritmo calcula a probabilidade de cada ponto de dados pertencer a cada um dos clusters. Isso é feito usando a distribuição gaussiana de cada cluster e atualizando as probabilidades usando o teorema de Bayes.\n",
        "   \n",
        "   - **Maximization Step (Passo de Maximização)**: Com as probabilidades atualizadas, o algoritmo então recalcula os parâmetros das distribuições gaussianas para maximizar a probabilidade de observar os dados. Isso geralmente envolve ajustar as médias, covariâncias e pesos (ou seja, proporções de pontos de dados atribuídos a cada cluster).\n",
        "\n",
        "3. **Convergência**: O processo de Expectation-Maximization é repetido iterativamente até que os parâmetros do modelo converjam para valores onde a mudança nos parâmetros se torna insignificante entre iterações consecutivas ou até que um critério de parada predefinido seja atingido.\n",
        "\n",
        "4. **Atribuição de Clusters**: Depois que o algoritmo convergiu, os pontos de dados são atribuídos ao cluster com a maior probabilidade de pertencimento calculada durante a etapa de Expectation.\n",
        "\n",
        "5. **Avaliação**: Por fim, é importante avaliar a qualidade dos clusters obtidos. Isso pode ser feito usando métricas de avaliação interna, como coeficiente de silhueta, ou métricas externas, dependendo dos dados e dos objetivos do problema.\n",
        "\n",
        "O processo de clusterização pelo método GMM é iterativo e probabilístico, o que significa que ele pode ser robusto e eficaz para dados complexos que não podem ser adequadamente agrupados usando métodos mais simples, como o k-means.\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:753/1*lTv7e4Cdlp738X_WFZyZHA.png\" alt=\"Imagem\" width=\"600\" height=\"300\">\n"
      ],
      "metadata": {
        "id": "-d7CTtxp0Ecs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parametros"
      ],
      "metadata": {
        "id": "IoNUhXBg_GZL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "class sklearn.mixture.GaussianMixture(n_components=1, *, covariance_type='full', tol=0.001, reg_covar=1e-06, max_iter=100, n_init=1, init_params='kmeans', weights_init=None, means_init=None, precisions_init=None, random_state=None, warm_start=False, verbose=0, verbose_interval=10)\n",
        "```\n",
        "\n",
        "1. **Número de Componentes (Clusters)**: Este é o número de distribuições gaussianas que serão usadas para modelar os dados. Cada componente representa um cluster potencial. Escolher o número certo de clusters é crucial e pode exigir alguma investigação e compreensão do conjunto de dados.\n",
        "\n",
        "2. **Inicialização dos Parâmetros**: Os parâmetros iniciais do GMM incluem as médias, covariâncias e pesos de cada componente gaussiano. Esses parâmetros podem ser inicializados de várias maneiras, como aleatoriamente ou usando métodos de inicialização específicos, como o algoritmo k-means++.\n",
        "\n",
        "3. **Critério de Convergência**: É necessário definir um critério de parada para o algoritmo, que determina quando o processo de otimização dos parâmetros converge. Isso pode ser baseado em uma tolerância para a mudança nos parâmetros ou no número máximo de iterações.\n",
        "\n",
        "4. **Covariance Type (Tipo de Covariância)**: Este parâmetro determina a forma da matriz de covariância para cada componente gaussiano. As opções comuns incluem:\n",
        "   - **full**: Covariância completa, onde cada componente tem sua própria matriz de covariância completa.\n",
        "   - **tied**: Covariância compartilhada, onde todos os componentes têm a mesma matriz de covariância.\n",
        "   - **diag**: Covariância diagonal, onde cada componente tem sua própria matriz de covariância diagonal.\n",
        "   - **spherical**: Covariância esférica, onde cada componente tem uma única variância.\n",
        "\n",
        "5. **Algoritmo de Otimização**: O GMM geralmente é otimizado usando o algoritmo Expectation-Maximization (EM). Existem variações do EM que podem ser usadas, como o algoritmo EM padrão ou o EM variacional.\n",
        "\n",
        "6. **Parâmetros de Regularização (se aplicável)**: Em alguns casos, pode ser necessário aplicar técnicas de regularização para evitar overfitting ou instabilidade numérica. Por exemplo, você pode querer adicionar uma pequena quantidade de diagonal à matriz de covariância para estabilizar o cálculo.\n",
        "\n",
        "A escolha adequada desses parâmetros depende da natureza do conjunto de dados e dos objetivos do agrupamento. Experimentação e validação cruzada geralmente são necessárias para determinar os melhores parâmetros para o seu caso específico."
      ],
      "metadata": {
        "id": "ViK8yHnx_JfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code"
      ],
      "metadata": {
        "id": "PxQ8842M0ENm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# Criar o objeto GaussianMixture\n",
        "gmm_model = GaussianMixture(n_components=6)\n",
        "\n",
        "# Ajustar o modelo aos dados\n",
        "gmm_model.fit(Ctreino)\n",
        "\n",
        "# Atribuições de cluster\n",
        "clusters = gmm_model.predict(Ctreino)\n",
        "\n",
        "# Adicionando os clusters de volta ao DataFrame original\n",
        "Ctreino2 = Ctreino.copy()\n",
        "Ctreino2['Cluster'] = clusters"
      ],
      "metadata": {
        "id": "hjE0eUpiQtMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ctreino2['Cluster'].value_counts()"
      ],
      "metadata": {
        "id": "G7mdyzJCq3mR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agglomerative Clustering"
      ],
      "metadata": {
        "id": "Um_m5IhZQtZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explicação"
      ],
      "metadata": {
        "id": "c5PcWhYT0KaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O método de Aglomerative Clustering é uma técnica de agrupamento hierárquico que começa com cada ponto de dados em seu próprio cluster e, em seguida, mescla gradualmente os clusters mais próximos uns dos outros até que todos os pontos de dados estejam em um único cluster.\n",
        "\n",
        "1. **Inicialização**: No início, cada ponto de dados é considerado como um cluster separado. Portanto, se você tiver N pontos de dados, haverá N clusters no início.\n",
        "\n",
        "2. **Cálculo da Similaridade entre Clusters**: Em cada etapa do processo, calcula-se a similaridade entre todos os pares de clusters. A similaridade pode ser medida de várias maneiras, como a distância entre os centróides dos clusters, a distância mínima entre os pontos de cada cluster (single linkage), a distância máxima entre os pontos de cada cluster (complete linkage), ou a média das distâncias entre os pontos de cada cluster (average linkage).\n",
        "\n",
        "3. **Mesclagem dos Clusters Mais Próximos**: Após calcular as similaridades entre os clusters, o par de clusters mais similares é mesclado em um novo cluster. A definição de \"mais similares\" depende da medida de similaridade escolhida.\n",
        "\n",
        "4. **Atualização da Matriz de Similaridade**: Após mesclar os clusters, a matriz de similaridade é atualizada para refletir a nova configuração de clusters. Isso significa que a similaridade entre o novo cluster e os clusters restantes precisa ser recalculada.\n",
        "\n",
        "5. **Repetição do Processo**: Os passos 3 e 4 são repetidos até que todos os pontos de dados estejam em um único cluster.\n",
        "\n",
        "6. **Construção da Árvore Hierárquica (Dendrograma)**: Durante o processo de mesclagem, uma árvore hierárquica é construída, chamada dendrograma, que mostra como os clusters foram mesclados ao longo do tempo. Isso pode ser útil para entender a estrutura hierárquica dos dados.\n",
        "\n",
        "7. **Determinação do Número de Clusters (Opcional)**: Você pode decidir onde cortar a árvore hierárquica para obter um determinado número de clusters. Isso pode ser feito usando critérios como a distância entre os clusters no dendrograma ou usando métodos como o Elbow Method.\n",
        "\n",
        "O método Aglomerative Clustering é relativamente simples de entender e implementar, e é especialmente útil quando a estrutura hierárquica dos dados é importante ou quando não se sabe a priori o número ideal de clusters.\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:740/1*VvOVxdBb74IOxxF2RmthCQ.png\" alt=\"Imagem\" width=\"600\" height=\"300\">\n"
      ],
      "metadata": {
        "id": "mquvlM5j0KV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parametros"
      ],
      "metadata": {
        "id": "M-uyckrDIZ8o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "class sklearn.cluster.AgglomerativeClustering(n_clusters=2, *, metric='euclidean', memory=None, connectivity=None, compute_full_tree='auto', linkage='ward', distance_threshold=None, compute_distances=False)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "1. **Método de Ligação (Linkage)**: O método de ligação é utilizado para calcular a similaridade entre clusters durante o processo de aglomeração. Alguns dos métodos de ligação mais comuns incluem:\n",
        "   - **Single Linkage**: Calcula a similaridade entre os dois clusters mais próximos.\n",
        "   - **Complete Linkage**: Calcula a similaridade entre os dois clusters mais distantes.\n",
        "   - **Average Linkage**: Calcula a similaridade entre todos os pares de pontos nos dois clusters e, em seguida, calcula a média dessas similaridades.\n",
        "   - **Ward's Linkage**: Minimiza a variância intra-cluster quando os clusters são mesclados.\n",
        "\n",
        "2. **Métrica de Distância**: A métrica de distância é usada para calcular a distância entre os pontos de dados ou os centróides dos clusters. Alguns exemplos de métricas de distância incluem a distância euclidiana, a distância de Manhattan, a distância de Minkowski, entre outras.\n",
        "\n",
        "3. **Critério de Fusão (Critério de Parada)**: Este parâmetro define o critério para parar o processo de aglomeração. Pode ser especificado como um número de clusters desejado ou uma distância máxima entre os clusters.\n",
        "\n",
        "4. **Matriz de Conectividade (Opcional)**: Uma matriz de conectividade pode ser fornecida para restringir quais clusters podem ser mesclados entre si. Isso pode ser útil em casos onde há conhecimento prévio sobre a estrutura dos dados.\n",
        "\n",
        "5. **Número de Clusters (Opcional)**: Embora não seja um parâmetro direto, o número de clusters pode ser inferido observando o dendrograma resultante do processo de aglomeração. Você pode decidir onde cortar o dendrograma para obter um número desejado de clusters.\n",
        "\n",
        "Estes são os principais parâmetros que podem ser ajustados no algoritmo de Aglomerative Clustering. A escolha adequada desses parâmetros depende da natureza dos dados e dos objetivos do agrupamento. Experimentação e validação cruzada são geralmente necessárias para determinar os melhores parâmetros para o seu caso específico."
      ],
      "metadata": {
        "id": "7T7bt1EpIadq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code"
      ],
      "metadata": {
        "id": "9JVcyZlx0KN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Criar o objeto AgglomerativeClustering\n",
        "agg_model = AgglomerativeClustering(n_clusters=6)\n",
        "\n",
        "# Ajustar o modelo aos dados\n",
        "agg_model.fit(Ctreino)\n",
        "\n",
        "# Atribuições de cluster\n",
        "clusters = agg_model.labels_\n",
        "\n",
        "# Adicionando os clusters de volta ao DataFrame original\n",
        "Ctreino2 = Ctreino.copy()\n",
        "Ctreino2['Cluster'] = clusters\n"
      ],
      "metadata": {
        "id": "XeoBko8WQwgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ctreino2['Cluster'].value_counts()"
      ],
      "metadata": {
        "id": "1kiU67uJrV7e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}